# ==========================================
# train_model_LGBM_v4.py
# åŠŸèƒ½ï¼šè‡ªå‹•åµæ¸¬ GPUã€è½‰æ›æ—¥æœŸæ¬„ä½ã€è¨“ç·´ LightGBM æ¨¡å‹
# ==========================================

import pandas as pd
import lightgbm as lgb
import joblib
import os
import time
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    f1_score, precision_score, recall_score,
    accuracy_score, confusion_matrix
)

# ======================
# è·¯å¾‘è¨­å®š
# ======================
train_file = "train_data_v4/train_data.csv"
output_dir = "train_data_v4"
os.makedirs(output_dir, exist_ok=True)

model_file = os.path.join(output_dir, "lightgbm_model.pkl")
feature_importance_file = os.path.join(output_dir, "feature_importance.csv")
threshold_file = os.path.join(output_dir, "best_threshold.json")

# ======================
# 1ï¸âƒ£ è®€å–è³‡æ–™
# ======================
print(f"ğŸ“‚ è®€å–è¨“ç·´è³‡æ–™ï¼š{train_file}")
df = pd.read_csv(train_file)

# ======================
# 2ï¸âƒ£ ä¿®æ­£æ—¥æœŸæ¬„ä½
# ======================
for col in ["first_txn", "last_txn"]:
    if col in df.columns:
        df[col] = pd.to_datetime(df[col], errors="coerce")
        df[col] = (df[col] - pd.Timestamp("1970-01-01")) // pd.Timedelta("1s")
        df[col] = df[col].fillna(0).astype("int64")
        print(f"ğŸ•’ å·²å°‡æ¬„ä½ {col} è½‰æ›ç‚º UNIX ç§’æ•¸æ ¼å¼")

# ======================
# 3ï¸âƒ£ æº–å‚™è³‡æ–™
# ======================
y = df["alert_flag"]
X = df.drop(columns=["acct_id", "alert_flag"], errors="ignore")

# æª¢æŸ¥éæ•¸å€¼æ¬„ä½
non_numeric = [c for c in X.columns if X[c].dtype not in ["int64", "float64", "bool"]]
if non_numeric:
    print(f"âš ï¸ ç§»é™¤éæ•¸å€¼æ¬„ä½ï¼š{non_numeric}")
    X = X.drop(columns=non_numeric)

print(f"âœ… æœ€çµ‚ç‰¹å¾µæ•¸é‡ï¼š{X.shape[1]}")

# ======================
# 4ï¸âƒ£ åˆ†å‰²è¨“ç·´ / é©—è­‰é›†
# ======================
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# ======================
# 5ï¸âƒ£ å˜—è©¦ä½¿ç”¨ GPU
# ======================
device_type = "cpu"
try:
    test_params = {"device": "gpu"}
    dtest = lgb.Dataset([[0, 1], [1, 0]], label=[0, 1])
    lgb.train(test_params, dtest, num_boost_round=1)
    device_type = "gpu"
    print("âš¡ GPU å¯ç”¨ï¼Œå°‡ä½¿ç”¨ GPU åŠ é€Ÿè¨“ç·´")
except Exception:
    print("ğŸ’¡ æœªåµæ¸¬åˆ° GPUï¼Œæ”¹ç”¨ CPU æ¨¡å¼")

# ======================
# 6ï¸âƒ£ LightGBM åƒæ•¸è¨­å®š
# ======================
params = {
    "objective": "binary",
    "metric": "binary_logloss",
    "boosting_type": "gbdt",
    "num_leaves": 63,
    "learning_rate": 0.05,
    "feature_fraction": 0.8,
    "bagging_fraction": 0.8,
    "bagging_freq": 5,
    "verbosity": -1,
    "device": device_type
}

# ======================
# 7ï¸âƒ£ æ¨¡å‹è¨“ç·´
# ======================
print(f"ğŸš€ é–‹å§‹è¨“ç·´ LightGBM æ¨¡å‹ ({device_type.upper()}) ...")
start = time.time()

train_data = lgb.Dataset(X_train, label=y_train)
val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)

callbacks = [
    lgb.early_stopping(stopping_rounds=100),
    lgb.log_evaluation(period=200)
]

model = lgb.train(
    params=params,
    train_set=train_data,
    num_boost_round=2000,
    valid_sets=[train_data, val_data],
    valid_names=["train", "valid"],
    callbacks=callbacks
)

end = time.time()
print(f"â±ï¸ è¨“ç·´å®Œæˆï¼Œè€—æ™‚ {end - start:.2f} ç§’")

# ======================
# 8ï¸âƒ£ æ¨¡å‹è©•ä¼°
# ======================
print("ğŸ“Š æ¨¡å‹è©•ä¼°ä¸­ ...")
y_pred_prob = model.predict(X_val, num_iteration=model.best_iteration)

thresholds = [i / 100 for i in range(30, 71, 5)]
best_f1, best_th = 0, 0.5
for th in thresholds:
    y_pred = (y_pred_prob >= th).astype(int)
    f1 = f1_score(y_val, y_pred)
    if f1 > best_f1:
        best_f1, best_th = f1, th

y_pred = (y_pred_prob >= best_th).astype(int)
precision = precision_score(y_val, y_pred)
recall = recall_score(y_val, y_pred)
acc = accuracy_score(y_val, y_pred)
cm = confusion_matrix(y_val, y_pred)

print("\nâœ… è©•ä¼°çµæœï¼š")
print(f"Best Threshold : {best_th:.3f}")
print(f"F1 Score       : {best_f1:.4f}")
print(f"Precision      : {precision:.4f}")
print(f"Recall         : {recall:.4f}")
print(f"Accuracy       : {acc:.4f}")
print("Confusion Matrix:")
print(cm)

# ======================
# 9ï¸âƒ£ å„²å­˜æ¨¡å‹èˆ‡ç‰¹å¾µé‡è¦åº¦
# ======================
joblib.dump(model, model_file)
print(f"\nğŸ’¾ æ¨¡å‹å·²å„²å­˜è‡³ï¼š{model_file}")

feature_importance = pd.DataFrame({
    "feature": X.columns,
    "importance": model.feature_importance()
}).sort_values(by="importance", ascending=False)
feature_importance.to_csv(feature_importance_file, index=False)
print(f"ğŸ“Š ç‰¹å¾µé‡è¦åº¦å·²è¼¸å‡ºè‡³ï¼š{feature_importance_file}")

# å„²å­˜æœ€ä½³é–€æª»
import json
with open(threshold_file, "w", encoding="utf-8") as f:
    json.dump({"best_threshold": best_th}, f, indent=2)
print(f"ğŸ’¾ æœ€ä½³é–¾å€¼å·²å„²å­˜ï¼š{threshold_file}")


