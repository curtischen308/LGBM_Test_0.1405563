# ===============================================================
# build_features_v4_behavior_progress.py
# åŠŸèƒ½ï¼šé€²éšŽå¸³è™Ÿç‰¹å¾µå»ºç«‹ï¼ˆå«æ—¥æœŸä¿®æ­£ + é€²åº¦é¡¯ç¤ºï¼‰
# ===============================================================

import os
import numpy as np
import pandas as pd
from scipy.stats import entropy

print("ðŸ“‚ è®€å–äº¤æ˜“è³‡æ–™ï¼šdataset/acct_transaction.csv")
df = pd.read_csv("dataset/acct_transaction.csv")

print("ðŸ§¹ è³‡æ–™æ¸…ç†ä¸­ ...")
df["txn_amt"] = pd.to_numeric(df["txn_amt"], errors="coerce").fillna(0)

# å°‡ä»¥ã€Œç¬¬1å¤©ç‚º1ã€çš„æ—¥åºè½‰ç‚ºæ—¥æœŸ
base_date = pd.Timestamp("2020-01-01")
df["txn_date"] = pd.to_datetime(base_date + pd.to_timedelta(df["txn_date"].astype(float) - 1, unit="D"), errors="coerce")

# å°‡æ™‚é–“æ–‡å­—è½‰æ›ç‚ºç§’æ•¸
if df["txn_time"].dtype == "object":
    t = pd.to_datetime(df["txn_time"], errors="coerce", format="%H:%M:%S")
    df["txn_time"] = t.dt.hour * 3600 + t.dt.minute * 60 + t.dt.second
else:
    df["txn_time"] = pd.to_numeric(df["txn_time"], errors="coerce").fillna(0)

df["hour"] = (df["txn_time"] // 3600).astype(int)
df["is_self_txn"] = df["is_self_txn"].fillna("UNK")

print("ðŸ“Š å½™æ•´å¸³è™Ÿç‰¹å¾µä¸­ ...")
g = df.groupby("from_acct")
features = g.agg({
    "txn_amt": ["count", "sum", "mean", "max", "min", "std"],
    "to_acct": "nunique",
    "channel_type": "nunique",
    "txn_date": ["min", "max"]
})
features.columns = [
    "txn_count", "total_amt", "avg_amt", "max_amt", "min_amt", "std_amt",
    "to_acct_nunique", "channel_nunique", "first_txn", "last_txn"
]
features["std_amt"] = features["std_amt"].fillna(0)

# === é€²éšŽç‰¹å¾µ ===
print("ðŸ§  å»ºç«‹é€²éšŽè¡Œç‚ºç‰¹å¾µ ...")
features["active_days"] = (features["last_txn"] - features["first_txn"]).dt.days + 1
features["txn_per_day"] = features["txn_count"] / features["active_days"].replace(0, 1)

print("ðŸ§© è¨ˆç®— night_txn_ratio ...")
night_ratio = df[(df["hour"] >= 0) & (df["hour"] <= 6)].groupby("from_acct").size() / g.size()
features["night_txn_ratio"] = night_ratio.fillna(0)

print("ðŸ§© è¨ˆç®— self_txn_ratio ...")
self_ratio = (df["is_self_txn"] == "Y").groupby(df["from_acct"]).mean()
features["self_txn_ratio"] = self_ratio.fillna(0)

print("ðŸ§© è¨ˆç®— weekday_ratio ...")
features["weekday_ratio"] = g["txn_date"].apply(lambda x: np.mean(x.dt.dayofweek < 5) if len(x) > 0 else 0)

print("ðŸ§© è¨ˆç®— peak_hour_ratio ...")
features["peak_hour_ratio"] = g["hour"].apply(lambda x: np.mean((x >= 9) & (x <= 17)) if len(x) > 0 else 0)

print("ðŸ§© è¨ˆç®— txn_hour_entropy ...")
features["txn_hour_entropy"] = g["hour"].apply(lambda x: entropy(x.value_counts(normalize=True), base=2))

print("ðŸ§© è¨ˆç®— channel_entropy ...")
features["channel_entropy"] = g["channel_type"].apply(lambda x: entropy(x.value_counts(normalize=True), base=2))

print("ðŸ§© è¨ˆç®— q90_over_meann & median_over_mean ...")
features["q90_over_meann"] = g["txn_amt"].apply(lambda x: np.percentile(x, 90) / (x.mean() + 1e-6))
features["median_over_mean"] = g["txn_amt"].apply(lambda x: x.median() / (x.mean() + 1e-6))

print("ðŸ§© è¨ˆç®— most_used_channel_ratio ...")
features["most_used_channel_ratio"] = g["channel_type"].apply(lambda x: x.value_counts(normalize=True).max() if len(x) > 0 else 0)

# è¼¸å‡º
os.makedirs("feature_data_v4", exist_ok=True)
features.reset_index(inplace=True)
features.rename(columns={"from_acct": "acct_id"}, inplace=True)
features.to_csv("feature_data_v4/account_features.csv", index=False)

print("âœ… å·²è¼¸å‡ºå¸³è™Ÿç‰¹å¾µæª”ï¼šfeature_data_v4/account_features.csv")
print(f"ðŸ“Š ç‰¹å¾µç¸½æ•¸ï¼š{features.shape[1]}")
